{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ea0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import uuid\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ec6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant with memory capabilities.\n",
    "If user-specific memory is available, use it to personalize \n",
    "your responses based on what you know about the user.\n",
    "\n",
    "Your goal is to provide relevant, friendly, and tailored \n",
    "assistance that reflects the user’s preferences, context, and past interactions.\n",
    "\n",
    "If the user’s name or relevant personal context is available, always personalize your responses by:\n",
    "    – Always Address the user by name (e.g., \"Sure, Aayushmaan...\") when appropriate\n",
    "    – Referencing known projects, tools, or preferences (e.g., \"your MCP server python based project\")\n",
    "    – Adjusting the tone to feel friendly, natural, and directly aimed at the user\n",
    "\n",
    "Avoid generic phrasing when personalization is possible.\n",
    "\n",
    "Use personalization especially in:\n",
    "    – Greetings and transitions\n",
    "    – Help or guidance tailored to tools and frameworks the user uses\n",
    "    – Follow-up messages that continue from past context\n",
    "\n",
    "Always ensure that personalization is based only on known user details and not assumed.\n",
    "\n",
    "In the end suggest 3 relevant further questions based on the current response and user profile\n",
    "\n",
    "The user’s memory (which may be empty) is provided as: {user_details_content}\n",
    "\"\"\"\n",
    "\n",
    "MEMORY_PROMPT = \"\"\"You are responsible for updating and maintaining accurate user memory.\n",
    "\n",
    "CURRENT USER DETAILS (existing memories):\n",
    "{user_details_content}\n",
    "\n",
    "TASK:\n",
    "- Review the user's latest message.\n",
    "- Extract user-specific info worth storing long-term (identity, stable preferences, ongoing projects/goals).\n",
    "- For each extracted item, set is_new=true ONLY if it adds NEW information compared to CURRENT USER DETAILS.\n",
    "- If it is basically the same meaning as something already present, set is_new=false.\n",
    "- Keep each memory as a short atomic sentence.\n",
    "- No speculation; only facts stated by the user.\n",
    "- If there is nothing memory-worthy, return should_write=false and an empty list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea747371",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_llm = init_chat_model(\"gpt-4o\")\n",
    "\n",
    "class MemoryItem(BaseModel):\n",
    "    text: str = Field(description=\"Atomic user memory\")\n",
    "    is_new: bool = Field(description=\"True if new, false if duplicate\")\n",
    "\n",
    "class MemoryDecision(BaseModel):\n",
    "    should_write: bool\n",
    "    memories: List[MemoryItem] = Field(default_factory=list)\n",
    "\n",
    "memory_extractor = memory_llm.with_structured_output(MemoryDecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6467d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    ns = (\"user\", user_id, \"details\")\n",
    "\n",
    "    # existing memory (all items under namespace)\n",
    "    items = store.search(ns)\n",
    "    existing = \"\\n\".join(it.value.get(\"data\", \"\") for it in items) if items else \"(empty)\"\n",
    "\n",
    "    # latest user message\n",
    "    last_text = state[\"messages\"][-1].content\n",
    "\n",
    "    decision: MemoryDecision = memory_extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=MEMORY_PROMPT.format(user_details_content=existing)),\n",
    "            {\"role\": \"user\", \"content\": last_text},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if decision.should_write:\n",
    "        for mem in decision.memories:\n",
    "            if mem.is_new and mem.text.strip():\n",
    "                store.put(ns, str(uuid.uuid4()), {\"data\": mem.text.strip()})\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73738b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm   = init_chat_model(\"gpt-4o\")\n",
    "\n",
    "def chat_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    ns = (\"user\", user_id, \"details\")\n",
    "\n",
    "    items = store.search(ns)\n",
    "    user_details = \"\\n\".join(it.value.get(\"data\", \"\") for it in items) if items else \"\"\n",
    "\n",
    "    system_msg = SystemMessage(\n",
    "        content=SYSTEM_PROMPT_TEMPLATE.format(user_details_content=user_details or \"(empty)\")\n",
    "    )\n",
    "\n",
    "    response = chat_llm.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09915d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1122297f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"remember\", remember_node)\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "builder.add_edge(START, \"remember\")\n",
    "builder.add_edge(\"remember\", \"chat\")\n",
    "builder.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a072416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Aayushmaan! Think of the transformer architecture as a smart way for computers to understand and generate language, much like how you follow a conversation. It’s a type of neural network that’s great at processing sequences of data, such as sentences.\n",
      "\n",
      "1. **Input Representation**: First, it converts words into numbers using something like word embeddings, so they can be processed by the computer.\n",
      "\n",
      "2. **Self-Attention Mechanism**: This part is like a highlighter that helps the model focus on certain parts of the input sentence. For example, if you’re reading about a Formula 1 race and the sentence is “Lewis Hamilton won the race,” the model can focus more on “won” and “race” to understand the context better.\n",
      "\n",
      "3. **Feed-Forward Networks**: After the highlight from the self-attention, the model processes the information further through layers called feed-forward networks, which help in making sophisticated decisions or predictions.\n",
      "\n",
      "4. **Layers and Transformers**: The model has multiple layers of these self-attention and feed-forward networks. More layers mean it can understand more complex patterns, kind of like a deep discussion about race strategies in a Formula 1 pit stop.\n",
      "\n",
      "5. **Positional Encoding**: Transformers also use positional encoding to understand the order of the words. This is essential since \"Hamilton leads Bottas\" and \"Bottas leads Hamilton\" mean very different things.\n",
      "\n",
      "Transformers are incredibly powerful, leading to breakthrough models like BERT and GPT, which are excellent at language-related tasks.\n",
      "\n",
      "How does that sound? If you're interested in applying this to your own projects or learning more, I can help with that too!\n",
      "\n",
      "Here are some related questions you might find interesting:\n",
      "1. How could transformers be used in analyzing Formula 1 data?\n",
      "2. What benefits do transformers have over previous models like LSTM?\n",
      "3. Are there any specific use cases of transformers in industries outside of language processing?\n",
      "\n",
      "--- Stored Memories (from Postgres) ---\n",
      "Aayushmaan likes Formula 1.\n",
      "The user's name is Aayushmaan.\n"
     ]
    }
   ],
   "source": [
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    # IMPORTANT: run ONCE the first time you use this database\n",
    "    store.setup()\n",
    "\n",
    "    graph = builder.compile(store=store)\n",
    "\n",
    "    config = {\"configurable\": {\"user_id\": \"u1\"}}\n",
    "\n",
    "    graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Aayushmaan\"}]}, config)\n",
    "    graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I like F1\"}]}, config)\n",
    "\n",
    "    out = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain transformer architecture in simple terms\"}]}, config)\n",
    "    print(out[\"messages\"][-1].content)\n",
    "\n",
    "    print(\"\\n--- Stored Memories (from Postgres) ---\")\n",
    "    for it in store.search((\"user\", \"u1\", \"details\")):\n",
    "        print(it.value[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33325a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
